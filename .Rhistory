epochs <- 1000
cat("Training on complex nonlinear data...\n")
fit <- nn_train(Xtr, ytr, d = d, h1 = h1, lr = lr, epochs = epochs,
seed = 1234, verbose_every = 100)
N <- nrow(X)
W1 <- params$W1; b1 <- params$b1
W2 <- params$W2; b2 <- params$b2
## =========================
## 1) Neural net utilities
## =========================
sigmoid <- function(x) 1 / (1 + exp(-x))
dsigmoid <- function(x) {
s <- sigmoid(x)
s * (1 - s)
}
nn_init <- function(d, h1, seed = 1) {
set.seed(seed)
list(
W1 = matrix(rnorm(h1 * d, sd = 0.1), nrow = h1, ncol = d),
b1 = rep(0, h1),
W2 = matrix(rnorm(1 * h1, sd = 0.1), nrow = 1, ncol = h1),
b2 = 0
)
}
params <- nn_init(d=5, h1=10)
params
nn_forward <- function(X, params) {
N <- nrow(X)
W1 <- params$W1; b1 <- params$b1
W2 <- params$W2; b2 <- params$b2
A <- X %*% t(W1) + matrix(rep(b1, each = N), nrow = N)
H <- sigmoid(A)
yhat <- as.numeric(H %*% t(W2) + b2)
list(A = A, H = H, yhat = yhat)
}
nn_backward <- function(X, y, cache, params) {
A <- cache$A   # N x h1
H <- cache$H   # N x h1
yhat <- cache$yhat
N <- nrow(X)
W2 <- params$W2
e <- yhat - y
two_over_N <- 2.0 / N
dW2 <- matrix(two_over_N * (t(e) %*% H), nrow = 1)
db2 <- two_over_N * sum(e)
EW2 <- e %*% W2               # N x h1
dA  <- two_over_N * EW2 * dsigmoid(A)  # N x h1
dW1 <- t(dA) %*% X            # h1 x d
db1 <- colSums(dA)            # length h1
list(dW1 = dW1, db1 = db1, dW2 = dW2, db2 = db2)
}
nn_update <- function(params, grads, lr) {
params$W1 <- params$W1 - lr * grads$dW1
params$b1 <- params$b1 - lr * grads$db1
params$W2 <- params$W2 - lr * grads$dW2
params$b2 <- params$b2 - lr * grads$db2
params
}
mse_loss <- function(y, yhat) mean((y - yhat)^2)
nn_train <- function(X, y, d, h1 = 32, lr = 1e-2, epochs = 2000, seed = 1, verbose_every = 100) {
params <- nn_init(d, h1, seed)
losses <- numeric(epochs)
for (t in seq_len(epochs)) {
cache <- nn_forward(X, params)
loss  <- mse_loss(y, cache$yhat)
losses[t] <- loss
grads <- nn_backward(X, y, cache, params)
params <- nn_update(params, grads, lr)
if (!is.null(verbose_every) && verbose_every > 0 && (t %% verbose_every == 0)) {
cat(sprintf("Epoch %4d | MSE: %.6f\n", t, loss))
}
}
list(params = params, losses = losses)
}
make_complex_data <- function(N = 4000, d = 5, seed = 123) {
set.seed(seed)
X <- matrix(runif(N * d, min = -3, max = 3), nrow = N, ncol = d)
x1 <- X[,1]; x2 <- X[,2]; x3 <- X[,3]
# Nonlinear target with mixed effects
f_true <- sin(x1) * x2 + 0.5 * (x3 > 0) * x1^2 + 1.2 * exp(-0.2 * ((x1 - 1)^2 + (x2 + 1)^2))
# Add small interactions if dimensions > 3
if (d >= 4) f_true <- f_true + 0.4 * X[,4] * tanh(0.5 * x2)
if (d >= 5) f_true <- f_true + 0.3 * X[,5] * x1
# Heteroskedastic noise
sigma <- 0.2 + 0.2 * abs(x1 + x2) / 3
y <- f_true + rnorm(N, sd = sigma)
# 5% outliers
#idx <- sample.int(N, round(0.05 * N))
#y[idx] <- y[idx] + rnorm(length(idx), sd = 2)
list(X = X, y = y, f_true = f_true)
}
data <- make_complex_data(N = 5000, d = 5, seed = 11)
X <- data$X
y <- data$y
nn_forward(X, params)
forward_output <- nn_forward(X, params)
backward_grad <- nn_backward(X, y, forward_output, params)
backward_grad
data("mtcars")
View(mtcars)
?mtcars
nn_init <- function(d, h1, seed = 1) {
set.seed(seed)
list(
W1 = matrix(rnorm(h1 * d, sd = 0.1), nrow = h1, ncol = d),
b1 = rep(0, h1),
W2 = matrix(rnorm(1 * h1, sd = 0.1), nrow = 1, ncol = h1),
b2 = 0
)
}
## =========================
## 1) Neural net utilities
## =========================
sigmoid <- function(x) 1 / (1 + exp(-x))
dsigmoid <- function(x) {
# derivative
s <- sigmoid(x)
s * (1 - s)
}
nn_init <- function(d, h1, seed = 1) {
set.seed(seed)
list(
W1 = matrix(rnorm(h1 * d, sd = 0.1), nrow = h1, ncol = d),
b1 = rep(0, h1),
W2 = matrix(rnorm(1 * h1, sd = 0.1), nrow = 1, ncol = h1),
b2 = 0
)
}
params <- nn_init(d=5, h1=10)
params <- nn_init(d=5, h1=10)
params
numeric(10)
seq_len(11)
## =========================
## 1) Neural net utilities
## =========================
sigmoid <- function(x) 1 / (1 + exp(-x))
dsigmoid <- function(x) {
# derivative
s <- sigmoid(x)
s * (1 - s)
}
nn_init <- function(d, h1, seed = 1) {
set.seed(seed)
list(
W1 = matrix(rnorm(h1 * d, sd = 0.1), nrow = h1, ncol = d),
b1 = rep(0, h1),
W2 = matrix(rnorm(1 * h1, sd = 0.1), nrow = 1, ncol = h1),
b2 = 0
)
}
params <- nn_init(d=5, h1=10)
params
nn_forward <- function(X, params) {
N <- nrow(X)
W1 <- params$W1; b1 <- params$b1
W2 <- params$W2; b2 <- params$b2
A <- X %*% t(W1) + matrix(rep(b1, each = N), nrow = N)
# cat("dim of A", dim(A)) # printing message for debugging.
H <- sigmoid(A)
yhat <- as.numeric(H %*% t(W2) + b2)
list(A = A, H = H, yhat = yhat)
}
nn_backward <- function(X, y, cache, params) {
# cache = stored history from forward pass
A <- cache$A   # N x h1
H <- cache$H   # N x h1
yhat <- cache$yhat
N <- nrow(X)
W2 <- params$W2
e <- yhat - y # this is not y - y_hat
two_over_N <- 2.0 / N
dW2 <- matrix(two_over_N * (t(e) %*% H), nrow = 1)
db2 <- two_over_N * sum(e)
EW2 <- e %*% W2               # N x h1
dA  <- two_over_N * EW2 * dsigmoid(A)  # N x h1
dW1 <- t(dA) %*% X            # h1 x d
db1 <- colSums(dA)            # length h1
list(dW1 = dW1, db1 = db1, dW2 = dW2, db2 = db2)
}
nn_update <- function(params, grads, lr) {
# gradient descent
params$W1 <- params$W1 - lr * grads$dW1
params$b1 <- params$b1 - lr * grads$db1
params$W2 <- params$W2 - lr * grads$dW2
params$b2 <- params$b2 - lr * grads$db2
params
}
mse_loss <- function(y, yhat) mean((y - yhat)^2)
nn_train <- function(X, y, d, h1 = 32, lr = 1e-2, epochs = 2000, seed = 1, verbose_every = 100) {
params <- nn_init(d, h1, seed) # initialize the param
losses <- numeric(epochs) # placeholder for losses
for (t in seq_len(epochs)) {
cache <- nn_forward(X, params)
loss  <- mse_loss(y, cache$yhat)
losses[t] <- loss
grads <- nn_backward(X, y, cache, params)
params <- nn_update(params, grads, lr)
if (t %% verbose_every == 0) {
cat(sprintf("Epoch %4d | MSE: %.6f\n", t, loss))
}
}
list(params = params, losses = losses)
}
make_complex_data <- function(N = 4000, d = 5, seed = 123) {
set.seed(seed)
X <- matrix(runif(N * d, min = -3, max = 3), nrow = N, ncol = d)
x1 <- X[,1]; x2 <- X[,2]; x3 <- X[,3]
# Nonlinear target with mixed effects
f_true <- sin(x1) * x2 + 0.5 * (x3 > 0) * x1^2 + 1.2 * exp(-0.2 * ((x1 - 1)^2 + (x2 + 1)^2))
# Add small interactions if dimensions > 3
if (d >= 4) f_true <- f_true + 0.4 * X[,4] * tanh(0.5 * x2)
if (d >= 5) f_true <- f_true + 0.3 * X[,5] * x1
# Heteroskedastic noise
sigma <- 0.2 + 0.2 * abs(x1 + x2) / 3
y <- f_true + rnorm(N, sd = sigma)
# 5% outliers
#idx <- sample.int(N, round(0.05 * N))
#y[idx] <- y[idx] + rnorm(length(idx), sd = 2)
list(X = X, y = y, f_true = f_true)
}
data <- make_complex_data(N = 5000, d = 5, seed = 11)
data
forward_output <- nn_forward(X, params)
data <- make_complex_data(N = 5000, d = 5, seed = 11)
X <- data$X
y <- data$y
forward_output <- nn_forward(X, params)
backward_grad <- nn_backward(X, y, forward_output, params)
## ==========================================
## 4) Run experiment
## ==========================================
set.seed(2025)
d <- 5
N <- 10000
data <- make_complex_data(N = N, d = d, seed = 11)
X <- data$X
y <- data$y
# Train/test split
set.seed(7)
idx <- sample.int(N, size = floor(0.8 * N))
Xtr <- X[idx, , drop = FALSE]; ytr <- y[idx]
Xte <- X[-idx, , drop = FALSE]; yte <- y[-idx]
## =========================
## 1) Neural net utilities
## =========================
sigmoid <- function(x) 1 / (1 + exp(-x))
dsigmoid <- function(x) {
# derivative
s <- sigmoid(x)
s * (1 - s)
}
nn_init <- function(d, h1, seed = 1) {
set.seed(seed)
list(
W1 = matrix(rnorm(h1 * d, sd = 0.1), nrow = h1, ncol = d),
b1 = rep(0, h1),
W2 = matrix(rnorm(1 * h1, sd = 0.1), nrow = 1, ncol = h1),
b2 = 0
)
}
params <- nn_init(d=5, h1=10)
nn_forward <- function(X, params) {
N <- nrow(X)
W1 <- params$W1; b1 <- params$b1
W2 <- params$W2; b2 <- params$b2
A <- X %*% t(W1) + matrix(rep(b1, each = N), nrow = N)
# cat("dim of A", dim(A)) # printing message for debugging.
H <- sigmoid(A)
yhat <- as.numeric(H %*% t(W2) + b2)
list(A = A, H = H, yhat = yhat)
}
rm(params)
nn_forward <- function(X, params) {
N <- nrow(X)
W1 <- params$W1; b1 <- params$b1
W2 <- params$W2; b2 <- params$b2
A <- X %*% t(W1) + matrix(rep(b1, each = N), nrow = N)
# cat("dim of A", dim(A)) # printing message for debugging.
H <- sigmoid(A)
yhat <- as.numeric(H %*% t(W2) + b2)
list(A = A, H = H, yhat = yhat)
}
nn_backward <- function(X, y, cache, params) {
# cache = stored history from forward pass
A <- cache$A   # N x h1
H <- cache$H   # N x h1
yhat <- cache$yhat
N <- nrow(X)
W2 <- params$W2
e <- yhat - y # this is not y - y_hat
two_over_N <- 2.0 / N
dW2 <- matrix(two_over_N * (t(e) %*% H), nrow = 1)
db2 <- two_over_N * sum(e)
EW2 <- e %*% W2               # N x h1
dA  <- two_over_N * EW2 * dsigmoid(A)  # N x h1
dW1 <- t(dA) %*% X            # h1 x d
db1 <- colSums(dA)            # length h1
list(dW1 = dW1, db1 = db1, dW2 = dW2, db2 = db2)
}
nn_update <- function(params, grads, lr) {
# gradient descent
params$W1 <- params$W1 - lr * grads$dW1
params$b1 <- params$b1 - lr * grads$db1
params$W2 <- params$W2 - lr * grads$dW2
params$b2 <- params$b2 - lr * grads$db2
params
}
mse_loss <- function(y, yhat) mean((y - yhat)^2)
nn_train <- function(X, y, d, h1 = 32, lr = 1e-2, epochs = 2000, seed = 1, verbose_every = 100) {
params <- nn_init(d, h1, seed) # initialize the param
losses <- numeric(epochs) # placeholder for losses
for (t in seq_len(epochs)) {
cache <- nn_forward(X, params)
loss  <- mse_loss(y, cache$yhat)
losses[t] <- loss
grads <- nn_backward(X, y, cache, params)
params <- nn_update(params, grads, lr)
if (t %% verbose_every == 0) {
cat(sprintf("Epoch %4d | MSE: %.6f\n", t, loss))
}
}
list(params = params, losses = losses)
}
make_complex_data <- function(N = 4000, d = 5, seed = 123) {
set.seed(seed)
X <- matrix(runif(N * d, min = -3, max = 3), nrow = N, ncol = d)
x1 <- X[,1]; x2 <- X[,2]; x3 <- X[,3]
# Nonlinear target with mixed effects
f_true <- sin(x1) * x2 + 0.5 * (x3 > 0) * x1^2 + 1.2 * exp(-0.2 * ((x1 - 1)^2 + (x2 + 1)^2))
# Add small interactions if dimensions > 3
if (d >= 4) f_true <- f_true + 0.4 * X[,4] * tanh(0.5 * x2)
if (d >= 5) f_true <- f_true + 0.3 * X[,5] * x1
# Heteroskedastic noise
sigma <- 0.2 + 0.2 * abs(x1 + x2) / 3
y <- f_true + rnorm(N, sd = sigma)
# 5% outliers
#idx <- sample.int(N, round(0.05 * N))
#y[idx] <- y[idx] + rnorm(length(idx), sd = 2)
list(X = X, y = y, f_true = f_true)
}
## ==========================================
## 4) Run experiment
## ==========================================
set.seed(2025)
d <- 5
N <- 10000
data <- make_complex_data(N = N, d = d, seed = 11)
X <- data$X
y <- data$y
# Train/test split
set.seed(7)
idx <- sample.int(N, size = floor(0.8 * N))
Xtr <- X[idx, , drop = FALSE]; ytr <- y[idx]
Xte <- X[-idx, , drop = FALSE]; yte <- y[-idx]
# Hyperparams
h1 <- 32       # wider hidden layer for complex data
lr <- 1e-3     # smaller LR to stabilize with outliers
epochs <- 1000
cat("Training on complex nonlinear data...\n")
fit <- nn_train(Xtr, ytr, d = d, h1 = h1, lr = lr, epochs = epochs,
seed = 1234, verbose_every = 100)
fit
fit$params
# Evaluate
pred_tr <- nn_forward(Xtr, fit$params)$yhat
pred_te <- nn_forward(Xte, fit$params)$yhat
mse_tr <- mse_loss(ytr, pred_tr)
mse_te <- mse_loss(yte, pred_te)
cat(sprintf("Train MSE: %.6f\n", mse_tr))
cat(sprintf(" Test MSE: %.6f\n", mse_te))
plot(fit$losses, type = "l", xlab = "Epoch", ylab = "MSE", main = "Training Loss")
# Parameters
needle_length <- 1 # Length of the needle (l)
line_spacing <- 2   # Spacing between lines (d)
num_simulations <- 1e6  # Number of needle drops
set.seed(123)  # For reproducibility
x <- runif(num_simulations, 0, line_spacing/2) # Uniform [0, d/2]
theta <- runif(num_simulations, 0, pi/2)       # Uniform [0, pi/2]
x
theta
plot(theta[1:1000], x[1:1000])
# Check if the needle intersects any line
intersections <- (needle_length / 2) * sin(theta) > x
empirical_prob <- mean(intersections)  # Proportion of intersections
cat("Empirical probability of intersection:", empirical_prob, "\n")
cat("Estimation of pi:", 1/empirical_prob, "\n")
plot(theta[1:1000], x[1:1000], col = ifelse(intersections, "red", "black"))
log(2*pi)
setwd("~/Documents/GitHub/GraphFL")
nodes <- read_csv("data/word/nodes.csv")
library(ggplot2)
library(dplyr)
nodes <- read_csv("data/word/nodes.csv")
library(ggplot2)
library(dplyr)
nodes <- read_csv("data/word/nodes.csv")
library(readr)
nodes <- read_csv("data/word/nodes.csv")
View(nodes)
word_cluster <- readLines(file.choose()) # GFL cluster result
cluster_indices <- lapply(word_cluster, function(x) as.integer(strsplit(x, " ")[[1]]))
word_cluster
word_clusters <- lapply(cluster_indices, function(indices) {
filter(nodes, `# index` %in% indices) %>% pull(label)
})
word_clusters
# Name the list by cluster
names(word_clusters) <- paste0("Cluster_", seq_along(word_clusters))
cluster_indices
nodes
matched_nodes <- nodes %>%
filter(label %in% word_clusters[[1]])
matched_nodes
matched_nodes
print(matched_nodes)
as.integer(word_clusters[[1]] %in% nodes$label)
word_clusters[[1]] %in% nodes$label
nodes$value
View(nodes)
match(word_clusters[[1]], nodes$label)
nodes$value[match(word_clusters[[1]], nodes$label)]
nodes$value[match(word_clusters[[1]], nodes$label)]
nodes$value[match(word_clusters[[2]], nodes$label)]
nodes$value[match(word_clusters[[3]], nodes$label)]
nodes$value[match(word_clusters[[4]], nodes$label)]
word_clusters
nodes$value[match(word_clusters[[1]], nodes$label)]
word_clusters$Cluster_1
word_clusters$Cluster_4
nodes$value[match(word_clusters[[4]], nodes$label)]
# Example usage
word_clusters$Cluster_1
nodes$value[match(word_clusters[[1]], nodes$label)]
word_clusters$Cluster_2
nodes$value[match(word_clusters[[2]], nodes$label)]
which(nodes$value[match(word_clusters[[2]], nodes$label)] == 1)
nodes$value[match(word_clusters[[1]], nodes$label)]
# Example usage
word_clusters$Cluster_1
file.choose()
TS <- read_csv("data/word/word_time_series.csv")
plot(1:64, TS[1,])
View(TS)
plot(1:64, TS[1,2:65])
plot(1:64, TS[1,2:65], type='l')
View(TS)
plot(1:64, TS[9,2:65], type='l')
plot(1:64, TS[110,2:65], type='l')
cluster_indices
plot(1:64, TS[82,2:65], type='l')
plot(1:64, TS[95,2:65], type='l')
plot(1:64, TS[97,2:65], type='l')
library(tidyverse)
library(readr)
library(igraph)
nodes <- read_csv("data/word/nodes.csv")
edges <- read_csv("data/word/edges.csv")
g <- graph_from_data_frame(d = edges, vertices = nodes, directed = FALSE)
A <- as_adjacency_matrix(g, sparse = FALSE) # 112 by 112
colnames(A) <- rownames(A) <- nodes$label
View(nodes)
which(nodes$label == "family")
A[110,]
which(nodes$label == "aunt")
A[9,]
which(nodes$label == "bed")
A[83,]
which(nodes$label == "half")
A[96,]
library(ggplot2)
library(dplyr)
library(readr)
nodes <- read_csv("data/word/nodes.csv")
word_cluster <- readLines(file.choose()) # GFL cluster result
cluster_indices <- lapply(word_cluster, function(x) as.integer(strsplit(x, " ")[[1]]))
word_clusters <- lapply(cluster_indices, function(indices) {
filter(nodes, `# index` %in% indices) %>% pull(label)
})
word_clusters
which(nodes$label == "name")
A[98,]
cluster_indices <- lapply(word_cluster, function(x) as.integer(strsplit(x, " ")[[1]]))
word_clusters <- lapply(cluster_indices, function(indices) {
filter(nodes, `# index` %in% indices) %>% pull(label)
})
# Name the list by cluster
names(word_clusters) <- paste0("Cluster_", seq_along(word_clusters))
# Example usage
word_clusters$Cluster_1
nodes$value[match(word_clusters[[1]], nodes$label)]
A[8,]
View(A)
A[14,]
word_cluster
word_clusters
rowSums(A)
which(nodes$label == "bed")
A[83,]
which(nodes$label == "name")
A[98,]
which(nodes$label == "half")
A[96,]
word_clusters
which(nodes$label == "little")
A[18,]
which(nodes$label == "old")
A[3,]
which(nodes$label == "nothing")
A[93,]
rowSums(A) == 1
nodes$label[rowSums(A) == 1]
